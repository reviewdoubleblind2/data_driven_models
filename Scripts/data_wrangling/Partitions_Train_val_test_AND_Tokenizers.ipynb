{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose of this notebook\n",
    "This Jupyter is used to split the data into train, validation and test. It is also used to generate the tokenizers for sequence and BOW tfidf.\n",
    "\n",
    "**The notebook must be in the same folder as the results of top5 and quartiles notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "# test GPY tensor flow\n",
    "from tensorflow.python.client import device_lib\n",
    "print(tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None))# --> true\n",
    "# CUDA with tensorflow support GPU? --> true\n",
    "print(tf.test.is_built_with_cuda()) \n",
    "# GPU active? --> GPU and CPU\n",
    "print(device_lib.list_local_devices())\n",
    "from keras import backend as K\n",
    "print(K.tensorflow_backend._get_available_gpus())# Keras using GPUS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections \n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Representation = namedtuple('Representation', 'b0 b1 b1_int b1_iden b1_int_iden') # Tokenizers\n",
    "#X\n",
    "DataXModel = namedtuple('DataXModel', 'b0 b1 b1_int b1_iden b1_int_iden') # DataFrames single columns\n",
    "#Y \n",
    "DataYModel = namedtuple('DataYModel', 'b0 b1 b1_int b1_iden b1_int_iden')# DataFrame two columns (Binary and categorical)\n",
    "\n",
    "#Index\n",
    "IndexTrain = namedtuple('Index', 'b0 b1 b1_int b1_iden b1_int_iden') # DataFrame\n",
    "\n",
    "# This aproach can be binary and multi \n",
    "approach = 'multi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representations = ['b0','b1','b1_int','b1_iden','b1_int_iden']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def description_vocab(dataset,representations,approach):\n",
    "    \n",
    "    print(\"Dataset:\", dataset)\n",
    "    \n",
    "    multiple_list = collections.defaultdict(list) \n",
    "    df_index = pd.DataFrame(columns= ['ID','Type'])\n",
    "    \n",
    "    for rep in representations:\n",
    "        #read data\n",
    "        temp = pd.read_csv(f'{approach}_{rep}_{dataset}.csv') \n",
    "        \n",
    "        \n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=0.4, random_state=0)\n",
    "        print(\"First split train /rest\")\n",
    "        column_name = \"\"\n",
    "\n",
    "        if approach == 'binary':\n",
    "            column_name = 'VULN_N' \n",
    "        else:\n",
    "            column_name = 'TYPE_N' \n",
    "\n",
    "        X = temp['0'].to_frame()\n",
    "        y = temp[column_name].to_frame()\n",
    "\n",
    "        print(\"Original Shape:\",temp.shape)\n",
    "        sss.get_n_splits(X, y)  \n",
    "\n",
    "        for train_index, test_val_index in sss.split(X, y):\n",
    "            print(\"Train Shape:\",train_index.shape)\n",
    "            X_train, X = X.loc[train_index].copy(), X.loc[test_val_index].copy()\n",
    "            y_train, y = y.loc[train_index].copy(), y.loc[test_val_index].copy()\n",
    "            #add index to save\n",
    "            df = pd.DataFrame({'ID':train_index,'Type':np.full(len(train_index),0)})\n",
    "            df_index.append(df, ignore_index= True)        \n",
    "        \n",
    "        \n",
    "        #SAVE x list    \n",
    "        multiple_list['list_x_train'].append(X_train)\n",
    "        multiple_list['list_y_train'].append(y_train)\n",
    "        X = X.reset_index(drop= True).copy()\n",
    "        y = y.reset_index().copy()\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=0)\n",
    "        sss.get_n_splits(X, y[column_name])  \n",
    "\n",
    "        \n",
    "        for val_index, test_index in sss.split(X, y[column_name]):\n",
    "            print(\"val-test Shape:\",val_index.shape,\"-\", test_index.shape)\n",
    "            X_val, X_test = X.loc[val_index].copy(), X.loc[test_index].copy()\n",
    "            y_val, y_test = y.loc[val_index].copy(), y.loc[test_index].copy()            \n",
    "            #add index to save val \n",
    "            df = pd.DataFrame({'ID':y_val['index'],'Type':np.full(len(val_index),1)})\n",
    "            df_index.append(df, ignore_index=True)    \n",
    "\n",
    "            #add index to save test \n",
    "            df = pd.DataFrame({'ID':y_test['index'],'Type':np.full(len(test_index),2)})\n",
    "            df_index.append(df, ignore_index=True)\n",
    "\n",
    "            y_val, y_test = y_val[column_name].to_frame(),  y_test[column_name].to_frame()\n",
    "        \n",
    "        # Save Validation and test\n",
    "        multiple_list['list_x_val'].append(X_val)\n",
    "        multiple_list['list_y_val'].append(y_val)\n",
    "        multiple_list['list_x_test'].append(X_test)\n",
    "        multiple_list['list_y_test'].append(y_test)\n",
    "        multiple_list['list_index'].append(df_index)                \n",
    "                \n",
    "        print(\"Representation:-tokenizer\",rep)\n",
    "        \n",
    "        #create a Tokenizer object that ignore spaces and not punctuations\n",
    "            token_obj = Tokenizer(filters=' ', lower= False,oov_token='OOVTOKEN')\n",
    "        token_obj.fit_on_texts(X_train['0'])\n",
    "        multiple_list['list_rep'].append(token_obj)\n",
    "        \n",
    "        print('length vocab:', len(token_obj.word_index))\n",
    "        print('length Docs (# functions):', token_obj.document_count)\n",
    "                \n",
    "                      \n",
    "        print(\" \")\n",
    "        print(\" \")\n",
    "    \n",
    "    #named tuples\n",
    "    Reps = Representation(*multiple_list['list_rep'])\n",
    "    X_train = DataXModel(*multiple_list['list_x_train'])\n",
    "    X_val = DataXModel(*multiple_list['list_x_val'])    \n",
    "    X_test = DataXModel(*multiple_list['list_x_test'])\n",
    "    Y_train = DataYModel(*multiple_list['list_y_train'])\n",
    "    Y_val = DataYModel(*multiple_list['list_y_val'])\n",
    "    Y_test = DataYModel(*multiple_list['list_y_test'])\n",
    "    Index_train = IndexTrain(*multiple_list['list_index'])\n",
    "    \n",
    "        \n",
    "    return (Reps,X_train, X_val, X_test, Y_train, Y_val, Y_test, Index_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(\"BOW\")\n",
    "except OSError:\n",
    "    print (\"Creation of the directory /BOW failed\")\n",
    "try:\n",
    "    os.mkdir(\"Tokenizer\")\n",
    "except OSError:\n",
    "    print (\"Creation of the directory \\Tokenizer failed\")\n",
    "try:\n",
    "    os.mkdir(\"Data_train_test\")\n",
    "except OSError:\n",
    "    print (\"Creation of the directory \\Data_train_test failed\")\n",
    "try:\n",
    "    os.mkdir(\"Index\")\n",
    "except OSError:\n",
    "    print (\"Creation of the directory \\Index failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocab description: RUSSELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer_Russel, x_train_Russel, x_val_Russel, x_test_Russel, y_train_Russel, y_val_Russel, y_test_Russel, index_Russel = description_vocab('Russell',representations,approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_Russel.b0.TYPE_N.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocab description: OUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer_OUR, x_train_OUR, x_val_OUR, x_test_OUR, y_train_OUR, y_val_OUR, y_test_OUR, index_OUR = description_vocab('OUR',representations,approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_OUR.b0.TYPE_N.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocab description: Juliet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer_juliet, x_train_juliet, x_val_juliet, x_test_juliet, y_train_juliet, y_val_juliet, y_test_juliet, index_juliet = description_vocab('Juliet',representations,approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_juliet.b0.TYPE_N.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer_juliet.b0.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE partitions and tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tokenizers(named_tupled,dataset,approach):    \n",
    "    for name, tokenizer in named_tupled._asdict().items():\n",
    "        name = f'Tokenizer/{approach}_{dataset}_{name}.pickle'\n",
    "        with open(name, 'wb') as handle:\n",
    "            pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "def save_data_train(named_tuple,dataset,data_type,approach):\n",
    "    for name, data in named_tuple._asdict().items():\n",
    "        name = f'Data_train_test/{approach}_{data_type}_{dataset}_{name}.csv'\n",
    "        data.to_csv(name,index=False)\n",
    "        \n",
    "def save_index(dataset,named_tuple,approach):\n",
    "    \n",
    "    for name, data in named_tuple._asdict().items():\n",
    "        name = f'Index/{approach}_{dataset}_{name}.csv'\n",
    "        data.to_csv(name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save named tuples of tokenizers by dataset and representation\n",
    "save_tokenizers(tokenizer_juliet,'juliet',approach)\n",
    "save_tokenizers(tokenizer_Russel,'Russell',approach)\n",
    "save_tokenizers(tokenizer_OUR,'OUR',approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JULIET\n",
    "save_data_train(x_train_juliet,'Juliet','Xtrain',approach)\n",
    "save_data_train(x_val_juliet,'Juliet','Xval',approach)\n",
    "save_data_train(x_test_juliet,'Juliet','Xtest',approach)\n",
    "save_data_train(y_train_juliet,'Juliet','Ytrain',approach)\n",
    "save_data_train(y_val_juliet,'Juliet','Yval',approach)\n",
    "save_data_train(y_test_juliet,'Juliet','Ytest',approach)\n",
    "save_index('Juliet',index_juliet,approach)\n",
    "\n",
    "#RUSSELL\n",
    "save_data_train(x_train_Russel,'Russell','Xtrain',approach)\n",
    "save_data_train(x_val_Russel,'Russell','Xval',approach)\n",
    "save_data_train(x_test_Russel,'Russell','Xtest',approach)\n",
    "save_data_train(y_train_Russel,'Russell','Ytrain',approach)\n",
    "save_data_train(y_val_Russel,'Russell','Yval',approach)\n",
    "save_data_train(y_test_Russel,'Russell','Ytest',approach)\n",
    "save_index('Russell',index_Russel,approach)\n",
    "\n",
    "#OUR\n",
    "save_data_train(x_train_OUR,'OUR','Xtrain',approach)\n",
    "save_data_train(x_val_OUR,'OUR','Xval',approach)\n",
    "save_data_train(x_test_OUR,'OUR','Xtest',approach)\n",
    "save_data_train(y_train_OUR,'OUR','Ytrain',approach)\n",
    "save_data_train(y_val_OUR,'OUR','Yval',approach)\n",
    "save_data_train(y_test_OUR,'OUR','Ytest',approach)\n",
    "save_index('OUR',index_OUR,approach)\n"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
